{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeqOcsBr72Jw76ksmI2NLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijayshankarrealdeal/GoogleColab/blob/main/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hTVWvGk7X-"
      },
      "source": [
        "import requests"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhTHA2_mdWa"
      },
      "source": [
        "url = \"https://www.statmt.org/europarl/v7/fr-en.tgz\""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMunABuOmiSK"
      },
      "source": [
        "r = requests.get(url,allow_redirects = True)\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNEgFypumz6u",
        "outputId": "45f266c4-efc4-48ce-8c92-4f30e569267f"
      },
      "source": [
        "open('fr-en.tgz', 'wb').write(r.content)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "202718517"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-C5RvownCz_"
      },
      "source": [
        "import tarfile"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9_-DQdZnFwg"
      },
      "source": [
        "r = tarfile.open(\"/content/fr-en.tgz\",'r')\n",
        "r.extractall(\"./\")\n",
        "r.close()\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCY6pkwOoosd"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quHJTELFpTth"
      },
      "source": [
        "with open('/content/europarl-v7.fr-en.en',mode = 'r',encoding='utf-8') as f:\n",
        "  europarl_en = f.read()\n",
        "###\n",
        "with open('/content/europarl-v7.fr-en.fr',mode = 'r',encoding='utf-8') as f:\n",
        "   europarl_fr = f.read()\n",
        "###\n",
        "with open('/content/Non-Breaking-Prefix.en',mode='r',encoding='utf-8') as f:\n",
        "  non_breaking_prefix_en = f.read()\n",
        "###\n",
        "with open('/content/Non-Breaking-Prefix.fr',mode='r',encoding='utf-8') as f:\n",
        "  non_breaking_prefix_fr = f.read()\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFH4JOdQtz6v"
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wa9PLR1yQgb"
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvnyT9074fUi"
      },
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_urAe4154sW"
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 # = 8171"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwOaGB1E6SI6"
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HqrOW29p2c9"
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PSK6o_iZqJy"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwlNar-_bNw6"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AArSLLf7cTLD"
      },
      "source": [
        "#Embadding"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjOdzoeMcatf"
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXwmzqeFcYaI"
      },
      "source": [
        "#Attention"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcRe5ytd57wi"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JNoYXUQ9ewR"
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Tf1rWbcbFb"
      },
      "source": [
        "#Encoder"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z_G8boLo7tQ"
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0c7WK12Ybax"
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHDSPQmeccvx"
      },
      "source": [
        "#Decoder"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khD7R4LVkKHq"
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzqx00sirsMV"
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ysDfqYIce5S"
      },
      "source": [
        "#Transformer"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDf1fD9hxY2H"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td6VdfWZTH5R"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "D_MODEL = 128 \n",
        "NB_LAYERS = 4 \n",
        "FFN_UNITS = 512 \n",
        "NB_PROJ = 8\n",
        "DROPOUT_RATE = 0.1 \n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sICTHcCgVJKc"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPxA5LnIXEm9"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ2guS_lcCNd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dac2780-07a1-4fa4-80ae-072df0013620"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.3275 Accuracy 0.0008\n",
            "Epoch 1 Batch 50 Loss 6.2215 Accuracy 0.0081\n",
            "Epoch 1 Batch 100 Loss 6.2015 Accuracy 0.0300\n",
            "Epoch 1 Batch 150 Loss 6.1184 Accuracy 0.0375\n",
            "Epoch 1 Batch 200 Loss 6.0141 Accuracy 0.0412\n",
            "Epoch 1 Batch 250 Loss 5.9039 Accuracy 0.0435\n",
            "Epoch 1 Batch 300 Loss 5.7863 Accuracy 0.0476\n",
            "Epoch 1 Batch 350 Loss 5.6612 Accuracy 0.0539\n",
            "Epoch 1 Batch 400 Loss 5.5342 Accuracy 0.0588\n",
            "Epoch 1 Batch 450 Loss 5.4152 Accuracy 0.0630\n",
            "Epoch 1 Batch 500 Loss 5.3104 Accuracy 0.0680\n",
            "Epoch 1 Batch 550 Loss 5.2096 Accuracy 0.0736\n",
            "Epoch 1 Batch 600 Loss 5.1109 Accuracy 0.0788\n",
            "Epoch 1 Batch 650 Loss 5.0202 Accuracy 0.0842\n",
            "Epoch 1 Batch 700 Loss 4.9344 Accuracy 0.0894\n",
            "Epoch 1 Batch 750 Loss 4.8502 Accuracy 0.0946\n",
            "Epoch 1 Batch 800 Loss 4.7685 Accuracy 0.0998\n",
            "Epoch 1 Batch 850 Loss 4.6937 Accuracy 0.1047\n",
            "Epoch 1 Batch 900 Loss 4.6237 Accuracy 0.1094\n",
            "Epoch 1 Batch 950 Loss 4.5548 Accuracy 0.1140\n",
            "Epoch 1 Batch 1000 Loss 4.4897 Accuracy 0.1184\n",
            "Epoch 1 Batch 1050 Loss 4.4280 Accuracy 0.1224\n",
            "Epoch 1 Batch 1100 Loss 4.3705 Accuracy 0.1262\n",
            "Epoch 1 Batch 1150 Loss 4.3184 Accuracy 0.1298\n",
            "Epoch 1 Batch 1200 Loss 4.2690 Accuracy 0.1332\n",
            "Epoch 1 Batch 1250 Loss 4.2217 Accuracy 0.1364\n",
            "Epoch 1 Batch 1300 Loss 4.1769 Accuracy 0.1396\n",
            "Epoch 1 Batch 1350 Loss 4.1345 Accuracy 0.1428\n",
            "Epoch 1 Batch 1400 Loss 4.0929 Accuracy 0.1458\n",
            "Epoch 1 Batch 1450 Loss 4.0531 Accuracy 0.1489\n",
            "Epoch 1 Batch 1500 Loss 4.0143 Accuracy 0.1519\n",
            "Epoch 1 Batch 1550 Loss 3.9758 Accuracy 0.1550\n",
            "Epoch 1 Batch 1600 Loss 3.9394 Accuracy 0.1579\n",
            "Epoch 1 Batch 1650 Loss 3.9036 Accuracy 0.1607\n",
            "Epoch 1 Batch 1700 Loss 3.8724 Accuracy 0.1633\n",
            "Epoch 1 Batch 1750 Loss 3.8402 Accuracy 0.1660\n",
            "Epoch 1 Batch 1800 Loss 3.8098 Accuracy 0.1687\n",
            "Epoch 1 Batch 1850 Loss 3.7797 Accuracy 0.1713\n",
            "Epoch 1 Batch 1900 Loss 3.7504 Accuracy 0.1738\n",
            "Epoch 1 Batch 1950 Loss 3.7223 Accuracy 0.1763\n",
            "Epoch 1 Batch 2000 Loss 3.6951 Accuracy 0.1786\n",
            "Epoch 1 Batch 2050 Loss 3.6680 Accuracy 0.1807\n",
            "Epoch 1 Batch 2100 Loss 3.6423 Accuracy 0.1826\n",
            "Epoch 1 Batch 2150 Loss 3.6162 Accuracy 0.1845\n",
            "Epoch 1 Batch 2200 Loss 3.5903 Accuracy 0.1864\n",
            "Epoch 1 Batch 2250 Loss 3.5643 Accuracy 0.1883\n",
            "Epoch 1 Batch 2300 Loss 3.5398 Accuracy 0.1900\n",
            "Epoch 1 Batch 2350 Loss 3.5158 Accuracy 0.1918\n",
            "Epoch 1 Batch 2400 Loss 3.4924 Accuracy 0.1936\n",
            "Epoch 1 Batch 2450 Loss 3.4694 Accuracy 0.1954\n",
            "Epoch 1 Batch 2500 Loss 3.4462 Accuracy 0.1971\n",
            "Epoch 1 Batch 2550 Loss 3.4237 Accuracy 0.1990\n",
            "Epoch 1 Batch 2600 Loss 3.4024 Accuracy 0.2008\n",
            "Epoch 1 Batch 2650 Loss 3.3807 Accuracy 0.2026\n",
            "Epoch 1 Batch 2700 Loss 3.3600 Accuracy 0.2044\n",
            "Epoch 1 Batch 2750 Loss 3.3392 Accuracy 0.2062\n",
            "Epoch 1 Batch 2800 Loss 3.3196 Accuracy 0.2080\n",
            "Epoch 1 Batch 2850 Loss 3.2995 Accuracy 0.2098\n",
            "Epoch 1 Batch 2900 Loss 3.2797 Accuracy 0.2115\n",
            "Epoch 1 Batch 2950 Loss 3.2611 Accuracy 0.2132\n",
            "Epoch 1 Batch 3000 Loss 3.2423 Accuracy 0.2149\n",
            "Epoch 1 Batch 3050 Loss 3.2242 Accuracy 0.2166\n",
            "Epoch 1 Batch 3100 Loss 3.2065 Accuracy 0.2183\n",
            "Epoch 1 Batch 3150 Loss 3.1889 Accuracy 0.2200\n",
            "Epoch 1 Batch 3200 Loss 3.1710 Accuracy 0.2216\n",
            "Epoch 1 Batch 3250 Loss 3.1531 Accuracy 0.2233\n",
            "Epoch 1 Batch 3300 Loss 3.1355 Accuracy 0.2249\n",
            "Epoch 1 Batch 3350 Loss 3.1185 Accuracy 0.2266\n",
            "Epoch 1 Batch 3400 Loss 3.1016 Accuracy 0.2283\n",
            "Epoch 1 Batch 3450 Loss 3.0852 Accuracy 0.2300\n",
            "Epoch 1 Batch 3500 Loss 3.0687 Accuracy 0.2317\n",
            "Epoch 1 Batch 3550 Loss 3.0531 Accuracy 0.2334\n",
            "Epoch 1 Batch 3600 Loss 3.0376 Accuracy 0.2352\n",
            "Epoch 1 Batch 3650 Loss 3.0222 Accuracy 0.2369\n",
            "Epoch 1 Batch 3700 Loss 3.0064 Accuracy 0.2386\n",
            "Epoch 1 Batch 3750 Loss 2.9917 Accuracy 0.2402\n",
            "Epoch 1 Batch 3800 Loss 2.9769 Accuracy 0.2418\n",
            "Epoch 1 Batch 3850 Loss 2.9624 Accuracy 0.2434\n",
            "Epoch 1 Batch 3900 Loss 2.9480 Accuracy 0.2450\n",
            "Epoch 1 Batch 3950 Loss 2.9339 Accuracy 0.2466\n",
            "Epoch 1 Batch 4000 Loss 2.9201 Accuracy 0.2482\n",
            "Epoch 1 Batch 4050 Loss 2.9063 Accuracy 0.2497\n",
            "Epoch 1 Batch 4100 Loss 2.8934 Accuracy 0.2513\n",
            "Epoch 1 Batch 4150 Loss 2.8803 Accuracy 0.2527\n",
            "Epoch 1 Batch 4200 Loss 2.8682 Accuracy 0.2540\n",
            "Epoch 1 Batch 4250 Loss 2.8563 Accuracy 0.2553\n",
            "Epoch 1 Batch 4300 Loss 2.8453 Accuracy 0.2566\n",
            "Epoch 1 Batch 4350 Loss 2.8344 Accuracy 0.2578\n",
            "Epoch 1 Batch 4400 Loss 2.8237 Accuracy 0.2589\n",
            "Epoch 1 Batch 4450 Loss 2.8132 Accuracy 0.2601\n",
            "Epoch 1 Batch 4500 Loss 2.8031 Accuracy 0.2612\n",
            "Epoch 1 Batch 4550 Loss 2.7930 Accuracy 0.2623\n",
            "Epoch 1 Batch 4600 Loss 2.7829 Accuracy 0.2634\n",
            "Epoch 1 Batch 4650 Loss 2.7731 Accuracy 0.2645\n",
            "Epoch 1 Batch 4700 Loss 2.7631 Accuracy 0.2655\n",
            "Epoch 1 Batch 4750 Loss 2.7532 Accuracy 0.2666\n",
            "Epoch 1 Batch 4800 Loss 2.7439 Accuracy 0.2677\n",
            "Epoch 1 Batch 4850 Loss 2.7342 Accuracy 0.2687\n",
            "Epoch 1 Batch 4900 Loss 2.7253 Accuracy 0.2697\n",
            "Epoch 1 Batch 4950 Loss 2.7163 Accuracy 0.2707\n",
            "Epoch 1 Batch 5000 Loss 2.7072 Accuracy 0.2717\n",
            "Epoch 1 Batch 5050 Loss 2.6980 Accuracy 0.2727\n",
            "Epoch 1 Batch 5100 Loss 2.6889 Accuracy 0.2737\n",
            "Epoch 1 Batch 5150 Loss 2.6804 Accuracy 0.2746\n",
            "Epoch 1 Batch 5200 Loss 2.6714 Accuracy 0.2756\n",
            "Epoch 1 Batch 5250 Loss 2.6625 Accuracy 0.2765\n",
            "Epoch 1 Batch 5300 Loss 2.6537 Accuracy 0.2774\n",
            "Epoch 1 Batch 5350 Loss 2.6449 Accuracy 0.2783\n",
            "Epoch 1 Batch 5400 Loss 2.6363 Accuracy 0.2791\n",
            "Epoch 1 Batch 5450 Loss 2.6278 Accuracy 0.2800\n",
            "Epoch 1 Batch 5500 Loss 2.6197 Accuracy 0.2808\n",
            "Epoch 1 Batch 5550 Loss 2.6116 Accuracy 0.2816\n",
            "Epoch 1 Batch 5600 Loss 2.6034 Accuracy 0.2824\n",
            "Epoch 1 Batch 5650 Loss 2.5954 Accuracy 0.2833\n",
            "Epoch 1 Batch 5700 Loss 2.5875 Accuracy 0.2841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-08910b26529c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                 epoch+1, batch, train_loss.result(), train_accuracy.result()))\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mckpt_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n\u001b[1;32m     28\u001b[0m                                                         ckpt_save_path))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ckpt_manager' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIW_QCP-onb3"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwJILcz-opJv"
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiv8GZahopr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f0e5dd-d0c3-4e91-d10d-0d0793757b2f"
      },
      "source": [
        "translate(\"hi\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: hi\n",
            "Predicted translation: Communication de la travaux\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}